---
title: "main2"
author: "Yitong Hu  (yh2875)"
date: "2017/3/12"
output: html_document
---

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

library("EBImage")
library("gbm")
```

```{r}
setwd("~/spr2017-proj3-group13")
dat_train<-read.csv("../data/sift_features.csv")
dat_train<-t(dat_train)

### pca features
### dat_train<-read.csv("../output/pca_features.csv")
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) run evaluation on an independent test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you maybe comparing very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup}
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
```


### Step 2: import training images class labels.

For the example of zip code digits, we code digit 9 as "1" and digit 7 as "0" for binary classification.

```{r train_label}
label_train <- read.csv("../data/labels.csv")
label_train <- label_train[1:2000,]
data<-as.data.frame(cbind(dat_train,label_train))
index <- 1:nrow(data)
trainindex <- sample(index, 0.8*nrow(data),replace=F)
testset <- data[-trainindex,]
trainset <-data[trainindex,]
```


### Step 3: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features.
  + Input: an R object of training sample labels.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifiers.
  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}

source("../lib/cross_validation.R")

if(run.cv){
  err_cv <- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(trainset[,1:(dim(trainset)[2]-1)], trainset[,(dim(trainset)[2])], model_values[k], K)
  }
  save(err_cv, file="../output/err_cv.RData")
}
```

Visualize cross-validation results. 

```{r cv_vis}
err_cv[,1]
mean(err_cv[,1])
plot(model_values, err_cv[,1])
```


* Choose the "best"" parameter value
```{r best_model}
model_best=model_values[1]
if(run.cv){
  model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
tm_train=NA
tm_train <- system.time(fit_train <- train(trainset[,1:5000], trainset[,5001], par_best))
#save(fit_train, file="../output/fit_train.RData")
```

### Step 5: Make prediction 
Feed the final training model with the completely holdout testing data. 
```{r}
tm_test=NA
if(run.test){
  tm_test <- system.time(pred_test <- test(fit_train, testset[,1:5000]))
}
sum(pred_test != testset[,5001])/length(testset[,5001])
```


### Advanced model


### load data
```{r}
setwd("~/spr2017-proj3-group13")
```

### 5000 sifted features
```{r}
###feature_sift<-read.csv("../data/sift_features.csv")
###feature_new<-t(feature_sift)
```

### 500 pac features
```{r}
feature<-read.csv("../output/pca_features2.csv")
feature_new<-feature
```



```{r}
label_train <- read.csv("../data/labels.csv")
y <- label_train[1:2000,]
data<-as.data.frame(cbind(feature_new,y))
```

### set apart train & test data

```{r}
index <- 1:nrow(data)
trainindex <- sample(index, 0.8*nrow(data),replace=F)
testset1 <- data[-trainindex,]
trainset1 <-data[trainindex,]
```


### train with svm
```{r}
source("../lib/train.R")
source("../lib/test.R")
mymodel<-train.svm.cv(trainset1)
```


```{r}
test.svm(mymodel,testset1)
```



